{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SVM的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性可分支持向量机：\n",
    "$$\n",
    "\\omega*x + b= 0\n",
    "$$\n",
    "决策函数：\n",
    "$$\n",
    "f(x) = sign(\\omega*x + b)\n",
    "$$\n",
    "函数间隔：\n",
    "$$\n",
    "\\hat{\\gamma }_i = y_i(\\omega \\cdot x_i+b)\n",
    "$$\n",
    "几何间隔：\n",
    "$$\n",
    "\\gamma_i =y_i(\\frac{\\omega }{\\left \\|\\omega   \\right \\|}\\cdot x_i+\\frac{b}{\\left \\|\\omega   \\right \\|})\n",
    "$$\n",
    "函数间隔和几何间隔关系：\n",
    "$$\n",
    "\\gamma_i  = \\frac{\\hat{\\gamma }_i}{\\left \\| \\omega \\right \\|}\n",
    "$$\n",
    "间隔最大化：\n",
    "$$\n",
    "\\underset{\\omega,b}{max}\\, \\: \\; \\; \\; \\; \\gamma\n",
    "$$\n",
    "$$\n",
    "s.t. \\: \\: y_i\\left (\\frac{\\omega }{\\left \\|\\omega   \\right \\|}\\cdot x_i+\\frac{b}{\\left \\|\\omega   \\right \\|}  \\right )\\geq \\gamma ,i=1,2,3\\cdot \\cdot \\cdot \\cdot N\n",
    "$$\n",
    "转换成几何间隔\n",
    "$$\n",
    "\\underset{\\omega,b}{max}\\, \\: \\; \\; \\; \\;\\frac{\\hat{\\gamma }}{\\left \\| \\omega \\right \\|}\n",
    "$$\n",
    "$$\n",
    "s.t. \\: \\: y_i(\\omega \\cdot x_i+b) \\geq \\hat{\\gamma },i=1,2,3\\cdot \\cdot \\cdot \\cdot N\n",
    "$$\n",
    "得到原始问题：\n",
    "$$\n",
    "\\underset{\\omega,b}{min}\\, \\: \\; \\; \\; \\;\\frac{1}{2}\\left \\|\\omega   \\right \\|^2\n",
    "$$\n",
    "$$\n",
    "s.t. \\: \\: y_i(\\omega \\cdot x_i+b)-1 \\geq 0,i=1,2,3\\cdot \\cdot \\cdot \\cdot N\n",
    "$$\n",
    "对偶问题：\n",
    "拉格朗日函数，对每一个不等式约束，引进拉格朗日乘子：\n",
    "$$\n",
    "L(\\omega ,b,\\alpha) = \\frac{1}{2}\\left \\|\\omega   \\right \\|^2 + \\sum_{i=1}^{N}\\alpha _i(1-y_i(\\omega \\cdot x_i+b)))),  \\: \\: \\: s.t. \\: \\: \\alpha_i\\geq 0,i=1,2,3\\cdot \\cdot \\cdot \\cdot N\n",
    "$$\n",
    "原始对偶问题是极大极小问题：\n",
    "$$\n",
    "\\underset{\\alpha}{max}\\: \\: \\underset{\\omega,b}{min}\\, \\: \\; \\; \\; \\;L(\\omega ,b,\\alpha)\n",
    "$$\n",
    "求关于\n",
    "$$\n",
    "\\underset{\\omega,b}{min}\\, \\: \\; \\; L(\\omega ,b,\\alpha)\n",
    "$$\n",
    "$$\n",
    "\\bigtriangledown _w L(\\omega ,b,\\alpha) = 0,\\: \\: \\: \n",
    "\\bigtriangledown _b L(\\omega ,b,\\alpha) = 0\n",
    "$$\n",
    "得\n",
    "$$\n",
    "w = \\sum_{i=1}^{N}\\alpha _iy_ix_i\n",
    "$$\n",
    "$$\n",
    "\\sum_{i=1}^{N}\\alpha _iy_i = 0\n",
    "$$\n",
    "代入得：\n",
    "$$\n",
    "\\underset{\\alpha}{max}L(\\omega ,b,\\alpha) = \\sum_{i=1}^{N}\\alpha _i - \\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha _i\\alpha _jy_iy_j(x_i\\cdot x_j）\n",
    "$$\n",
    "$$\n",
    "s.t. \\sum_{i=1}^{N}\\alpha _iy_i = 0，\\: \\: \\alpha_i\\geq 0,i=1,2,3\\cdot \\cdot \\cdot \\cdot N\n",
    "$$\n",
    "\n",
    "SMO算法：\n",
    "SMO算法先选择违背KKT条件最大的两个变量，利用约束条件，可化简成一个二次规划问题，且有闭式解。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SVM应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前支持向量机主要是应用于模式识别领域中的文本识别、中文分类和人脸识别等，同时也应用于许多工程技术和信息过滤的方面\n",
    "\n",
    "当前研究的热点还是对于支持向量机中算法的优化，包括解决SVM中QP问题的求解问题，另外就是如何更好的构造基于SVM的多类分类器，如何提高SVM的归纳能力和分类速度等，如何根据实际问题确定核函数是一个重要的研究热点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SVM优缺点 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优点\n",
    "* 1.专门针对有限样本情况，其目标是得到现有信息下的最优解，而不仅仅是样本数量趋于无穷大时的最优解\n",
    "* 2.算法最终转化为一个二次型寻优问题，理论上得到的是全局最优点，解决了在神经网络上无法避免的局部极值问题\n",
    "* 3.支持向量机算法能同时适用于稠密特征矢量与稀疏特征矢量两种情况，而其他一些分类算法不能同时满足两种情况\n",
    "* 4.支持向量机算法能够找到包含重要分类信息的支持向量，是强有力的增量学习和主动学习工具\n",
    "#### 缺点\n",
    "* 1.SVM算法对于大规模训练样本难以实施\n",
    "* 2.用SVM解决多分类问题存在困难\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SVM sklearn 参数学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn中SVM主要有LinearSVC、NuSVC和SVC三种方法：\n",
    "* 1.LinearSVC:\n",
    "* class LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
    "\n",
    "#### 参数：\n",
    "\n",
    "penalty:正则化参数，L1和L2两种参数可选，仅LinearSVC有\n",
    "\n",
    "loss：损失函数，有‘hinge’和‘Squared_hinge’两种可选，前者又称L1损失，后者为L2损失，默认为‘Squared_hinge’\n",
    "\n",
    "dual:是否转化为对偶问题求解，默认为True\n",
    "\n",
    "tol：残差收敛条件，默认为0.0001，与LR中的一致\n",
    "\n",
    "C：惩罚系数，用于控制损失函数的惩罚系数，类似于LR中的正则化系数\n",
    "\n",
    "multi_class:负责多分类问题中分类策略制定，有‘ovr’和‘crammer_singer’两个参数值可选，默认值为‘ovr’，‘ovr’的分类原则是将待分类中的某一类当做正类，其他的全部是负类\n",
    "\n",
    "crammer_singer:是直接针对目标函数设置多个参数值，最后进行优化，得到不同类别的参数值大小\n",
    "\n",
    "fit_intercept:是否计算截距，与LR模型中的意思一致\n",
    "\n",
    "class_weight:与其他模型中参数含义一样，也是用来处理不平衡样本数据的，可以直接以字典的形式指定不同类别的权重，也可以使用balanced参数值。\n",
    "\n",
    "verbose:是否冗余，默认是False。\n",
    "\n",
    "random_state:随机种子的大小。\n",
    "\n",
    "max_iter:最大迭代次数，默认是1000。\n",
    "\n",
    "#### 对象:\n",
    "\n",
    "coef_:各特征的系数（重要性）。\n",
    "\n",
    "intercept_:截距的大小（常数值）\n",
    "\n",
    "* 2.NuSVC:\n",
    "class sklearn.svm.NuSVC(nu=0.5,kernel='rbf',degree=3,gamma='auto',coef=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_dunction_shape='ovr',random_state=None)\n",
    "\n",
    "#### 参数：\n",
    "nu:训练误差部分的上限和支持向量部分的下限，取值在（0，1）之间，默认是0.5kernel:核函数，核函数是用来将非线性问题转化为线性问题的一种方法，默认是“rbf”核函数，常用的核函数有以下几种：\n",
    "\n",
    "表示解释linear线性核函数poly多项式核函数rbf高斯核函数sigmodsigmod核函数precomputed自定义核函数\n",
    "\n",
    "degree:当核函数是多项式核函数的时候，用来控制函数的最高次数。（多项式核函数是将低维的输入空间映射到高维的特征空间）\n",
    "\n",
    "gamma:核函数系数，默认是“auto”，即特征维度的倒数。\n",
    "\n",
    "coef0:核函数常数值(y=kx+b中的b值)，只有‘poly’和‘sigmoid’核函数有，默认值是0。\n",
    "\n",
    "max_iter:最大迭代次数，默认值是-1，即没有限制。\n",
    "\n",
    "probability:是否使用概率估计，默认是False。\n",
    "\n",
    "decision_function_shape:与’multi_class’参数含义类似。\n",
    "\n",
    "cache_size:缓冲大小，用来限制计算量大小，默认是200M。\n",
    "\n",
    "#### 对象:\n",
    "support_:以数组的形式返回支持向量的索引。\n",
    "\n",
    "support_vectors_:返回支持向量。\n",
    "\n",
    "n_support_:每个类别支持向量的个数。\n",
    "\n",
    "dual_coef_:支持向量系数。\n",
    "\n",
    "coef_:每个特征系数（重要性），只有核函数是LinearSVC的时候可用。\n",
    "\n",
    "intercept_:截距值（常数值）\n",
    "\n",
    "\n",
    "* 3.SVC:\n",
    "class sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
    "\n",
    "#### 参数：\n",
    "C:惩罚系数\n",
    "\n",
    "SVC和NuSVC方法基本一致，唯一区别就是损失函数的度量方式不同（NuSVC中的nu参数和SVC中的C参数）。\n",
    "\n",
    "#### 方法\n",
    "三种分类方法的方法基本一致，所以就一起来说啦。\n",
    "\n",
    "decision_function(X):获取数据集X到分离超平面的距离。\n",
    "\n",
    "fit(X, y):在数据集(X,y)上使用SVM模型。\n",
    "\n",
    "get_params([deep]):获取模型的参数。\n",
    "\n",
    "predict(X):预测数据值X的标签。\n",
    "\n",
    "score(X,y):返回给定测试集和对应标签的平均准确率\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 利用SVM模型结合 Tf-idf 算法进行文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/hanzhenlei767/Bag_of_Words_Meets_Bags_of_Popcorn\n",
    "* 这是我之前做的kaggle比赛，里面用到了NLP大部分的特征提取方法，以及机器学习算法和深度学习算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
