{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一. 前馈神经网络、网络层数、输入层、隐藏层、输出层、隐藏单元、激活函数的概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.前馈神经网络\n",
    "给定一组神经元，我们可以以神经元为节点来构建一个网络。不同的神经网络模型有着不同网络连接的拓扑结构。一种比较直接的拓扑结构是前馈网络。\n",
    "前馈神经网络（Feedforward Neural Network，FNN）是最早发明的简单人工神经网络。\n",
    "在前馈神经网络中，各神经元分别属于不同的层。每一层的神经元可以接收前一层神经元的信号，并产生信号输出到下一层。第0 层叫输入层，最后一层叫输出层，其它中间层叫做隐藏层。整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示。 一个简单的示意图如下：\n",
    "![](http://www.wildml.com/wp-content/uploads/2015/09/nn-from-scratch-3-layer-network.png)\n",
    "![](http://s0.wp.com/latex.php?zoom=2&latex=%5Cbegin%7Baligned%7D++z_1+%26+%3D+xW_1+%2B+b_1+%5C%5C++a_1+%26+%3D+%5Ctanh%28z_1%29+%5C%5C++z_2+%26+%3D+a_1W_2+%2B+b_2+%5C%5C++a_2+%26+%3D+%5Chat%7By%7D+%3D+%5Cmathrm%7Bsoftmax%7D%28z_2%29++%5Cend%7Baligned%7D&bg=ffffff&fg=000&s=0)\n",
    "![](http://s0.wp.com/latex.php?zoom=2&latex=W_1+%5Cin+%5Cmathbb%7BR%7D%5E%7B2%5Ctimes500%7D&bg=ffffff&fg=000&s=0)\n",
    "![](http://s0.wp.com/latex.php?zoom=2&latex=b_1+%5Cin+%5Cmathbb%7BR%7D%5E%7B500%7D&bg=ffffff&fg=000&s=0)\n",
    "![](http://s0.wp.com/latex.php?zoom=2&latex=W_2+%5Cin+%5Cmathbb%7BR%7D%5E%7B500%5Ctimes2%7D&bg=ffffff&fg=000&s=0)\n",
    "![](http://s0.wp.com/latex.php?zoom=2&latex=b_2+%5Cin+%5Cmathbb%7BR%7D%5E%7B2%7D&bg=ffffff&fg=000&s=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 激活函数的种类以及各自的提出背景、优缺点。（和线性模型对比，线性模型的局限性，去线性化）"
   ]
  },
  {
   "attachments": {
    "1.gif": {
     "image/gif": "R0lGODlhfQAWALMAAP///wAAALq6upiYmFRUVO7u7szMzNzc3ERERBAQEKqqqnZ2diIiIjIyMoiIiGZmZiH5BAEAAAAALAAAAAB9ABYAAAT+EMhJq7046827/5wwgGRpnqZoDUTrBgUqk8zjNI3zJPPGui3YhjDqGTUCgQShkDiOHiLHAOUUiiVs4iBpVjfUQYxyQIy/m0fvwOuoZg7qpKCeGBBttCY+WQQCBFwFBAEJdQADAQxeiAgYP0BCJ2UBFgJPEwoEehsNFQieFAELFaQUBJgcUjICjhahEqucFwNYEg8MFAUBhwACXBQMclMaA0obC6YVDoxbsxgIwH15iA2bE7YAu0fGHA3HFQbXbAYKLGgFCwMPNhLUTpUSvwiuiMsIAQ7SmQMLAuaBioHDsEuBAmYV2gxIIAdBthkHGMTYdewdonj2COQCcGDghl/+HBM0YShwg4IAwBKcARAPgbJFGB4AmUlAWYUGWIREtITSHoAFCkF4EWDxkUcLD5QlGCiRpTRJRgRIErBxZ7gABjrCA6Bg5YcF1y7IdIEDiE0JDI7tGgZAooE8CmAZcbDxZ50CFg8EeHOxACMSOD10y7BrzMmEAByERfDkKICxNFuclQB2QoNmFnYROHNysoddwBxTGIyBzQSXiANLQGlAdAkFrhJJi2YhHwUDWEkQvSghqw/XaOXJBSCurTR1D2UsYOaAWi0L9SQc8AzmgYJyCxQAl0Aag4DlkxE+KxVWW/TxHboDRn8z2wK27DV4BcF31oJQccXGf0aHvYLsDlAId8V+nIgxQQQAOw=="
    },
    "2.gif": {
     "image/gif": "R0lGODlhCgASALMAAP///wAAALq6upiYmFRUVO7u7szMzIiIiNzc3ERERHZ2djIyMqqqqhAQEGZmZiIiIiH5BAEAAAAALAAAAAAKABIAAARAEMhJqxRDDsJ7KBZAZKFRHWaFJEEICImrKO4ihEXAMAdDMQGEpAGSOGjDm+ShzKVyReAE0ZgkkMvLoiJQHLCSCAA7"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.线性激活函数\n",
    "可以使用单位函数g(z)=z g(z)=zg(z)=z作为激活函数 。但如果网络的每一层都是由线性变换组成，则网络作为整体也是线性的。这会降低网络的表达能力，因此线性激活函数较少使用。\n",
    "\n",
    "### 2.修正线性单元（relu）\n",
    "1. 修正线性单元采用激活函数g(z)=max{0,z} g(z)=max\\{0, z\\}g(z)=max{0,z}，它和线性单元非常类似，区别在于：修正线性单元在左侧的定义域上输出为零。\n",
    " 优点：采用基于梯度的优化算法时，非常易于优化。当修正线性单元处于激活状态时，导数为常数1 ；当修正线性单元处于非激活状态时，导数为常数0 。修正线性单元的二阶导数几乎处处为零。\n",
    "\n",
    " 缺点：无法通过基于梯度的方法学习那些使得修正线性单元处于非激活状态的参数，因为此时梯度为零。\n",
    "\n",
    "2. 对于修正线性单元![1.gif](attachment:1.gif)，初始化时可以将![2.gif](attachment:2.gif) 的所有元素设置成一个小的正值（如0.1），从而使得修正线性单元在初始时尽可能的对训练集中大多数输入呈现激活状态。\n",
    "\n",
    "3. 有许多修正线性单元的扩展存在，这些扩展保证了它们能在各个位置都保持非零的梯度。大多数扩展的表现与修正线性单元相差无几，偶尔表现的更好。\n",
    "\n",
    "### 3.sigmoid / tanh\n",
    "#####  在引入修正线性单元之前，大多数神经网络使用sigmoid函数g(z)=σ(z)或者双曲正切函数g(z)=tanh(z)作为激活函数。这两个激活函数密切相关，因为tanh(z)=2σ(2z)−1。\n",
    "#####   与修正线性单元不同，sigmoid单元和tanh单元在其大部分定义域内都饱和，仅仅当z在 0 附近才有一个较高的梯度，这会使得基于梯度的学习变得非常困难。因此，现在不鼓励将这两种单元用作前馈神经网络中的激活函数。\n",
    "* 如果选择了一个合适的代价函数（如对数似然函数）来抵消了sigmoid的饱和性，则这两种单元可以用作输出单元（而不是隐单元）。\n",
    "* 如果必须选用sigmoid激活函数时，tanh激活函数通常表现更佳。因为tanh函数在 0点附近近似于单位函数g(z)=z。\n",
    "\n",
    "##### sigmoid激活函数在前馈神经网络之外的神经网络中更为常见。有一些网络不能使用修正线性单元，因此sigmoid激活函数是个更好的选择，尽管它存在饱和问题。\n",
    "* 循环神经网络：修正线性单元会产生信息爆炸的问题。\n",
    "* 一些概率模型：要求输出在 0~1 之间。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 感知机相关；利用tensorflow等工具定义简单的几层网络（激活函数sigmoid），递归使用链式法则来实现反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)\n",
    "\n",
    "in_units = 784  #输入层节点数量\n",
    "h1_units = 300  #隐含层节点数量\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, in_units])    #定义输入x的placeholder\n",
    "keep_prob = tf.placeholder(tf.float32)  #定义dropout的比率\n",
    "\n",
    "#定义权重以及偏置\n",
    "W1 = tf.Variable(tf.truncated_normal([in_units, h1_units], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([h1_units]))\n",
    "W2 = tf.Variable(tf.zeros([h1_units, 10]))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#定义网络结构\n",
    "hidden1 = tf.nn.sigmoid(tf.add(tf.matmul(x, W1), b1))  #激活函数使用ReLU\n",
    "hidden1_drop = tf.nn.dropout(hidden1, keep_prob)    #在隐含层使用Dropout\n",
    "y = tf.nn.softmax(tf.add(tf.matmul(hidden1, W2), b2))   #使用softmax分类\n",
    "\n",
    "#定义损失函数以及确定使用损失函数最小化的优化算法Adagrad\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_enerty = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),reduction_indices=[1]))\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(cross_enerty)\n",
    "\n",
    "#定义会话以及初始化全部变量\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "#共使用3000个batch，每个batch100个样本进行训练模型\n",
    "for i in range(3000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    train_step.run({x: batch_xs, y_: batch_ys, keep_prob: 0.75})\n",
    "\n",
    "#计算模型的精度   \n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#对测试集进行精度计算\n",
    "print(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels,keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 深度学习中的正则化（参数范数惩罚：L1正则化、L2正则化；数据集增强；噪声添加；early stop；Dropout层）、正则化的介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 正则化常用于缓解模型过拟合。过拟合发生的原因是模型的容量过大，而正则化可以对模型施加某些限制，从而降低模型的有效容量。\n",
    "\n",
    "2. 目前有多种正则化策略。有些正则化策略是向模型添加额外的约束，如增加对参数的限制。这是对参数的硬约束。有些正则化策略是向目标函数增加额外项。这是对参数的软约束。\n",
    "3. 正则化策略代表了某种先验知识，即：倾向于选择简单的模型。\n",
    "\n",
    "4. 在深度学习中，大多数正则化策略都是基于对参数进行正则化。正则化以偏差的增加来换取方差的减少，而一个有效的正则化能显著降低方差，并且不会过度增加偏差。\n",
    "\n",
    "5. 在深度学习的实际应用中，不要因为害怕过拟合而采用一个小模型，推荐采用一个大模型并使用正则化。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 深度模型中的优化：参数初始化策略；自适应学习率算法（梯度下降、AdaGrad、RMSProp、Adam；优化算法的选择）；batch norm层（提出背景、解决什么问题、层在训练和测试阶段的计算公式）；layer norm层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 参数初始化策略\n",
    "1. 有些优化算法是非迭代的，可以直接解析求解最优解；有些优化算法是迭代的，但是它们是初始值无关的。深度学习不具有这两类性质，通常是迭代的，且与初始值相关。\n",
    "\n",
    "2. 深度学习中，大多数算法都受到初始值的影响。初始值能够决定：算法最终是否收敛、以及收敛时的收敛速度有多快、以及收敛到一个代价函数较高还是较低的值。\n",
    "\n",
    "3. 深度学习中，初始值也会影响泛化误差，而不仅仅是目标函数的最优化。因为如果选择一个不好的初始值，则最优化的结果会落在参数空间的一个较差的区域。此时会导致模型一个较差的泛化能力。\n",
    "\n",
    "4. 目前深度学习中，选择初始化策略是启发式的。\n",
    "\n",
    "    大多数初始化策略使得神经网络初始化时实现一些良好的性质。但是这些性质能否在学习过程中保持，难以保证。\n",
    "\n",
    "    有些初始化点从最优化的观点是有利的，但是从泛化误差的观点来看是不利的。设定一个好的初始化策略是困难的，因为神经网络最优化任务至今都未被很好理解。\n",
    "\n",
    "    对于初始点如何影响泛化误差的理论是空白的，几乎没有任何指导。\n",
    "\n",
    "5. 通常的参数初始化策略为：随机初始化权重，偏置通过启发式挑选常数，额外的参数也通过启发式挑选常数。\n",
    "\n",
    "6. 也可以使用机器学习来初始化模型的参数。在同样的数据集上，即使是用监督学习来训练一个不相关的任务，有时也能够得到一个比随机初始化更好的初始值。原因是：监督学习编码了模型初始参数分布的某些信息。\n",
    "\n",
    "\n",
    "#### 5.1.1 权重初始化\n",
    "1. 通常权重的初始化是从高斯分布或者均匀分布中挑选出来的值。\n",
    "   从高斯分布还是均匀分布中挑选，看起来似乎没有很大差别，实际上也没有被仔细研究。\n",
    "   该分布的范围（如均匀分布的上、下限）对优化结果和泛化能力有很大的影响。\n",
    "2. 初始权重的大小很重要，下面的因素决定了权重的初始值的大小：\n",
    "   更大的初始权重具有更强的破坏对称性的作用，有助于避免冗余的单元。\n",
    "   更大的初始权重也有助于避免梯度消失。\n",
    "   更大的初始权重也容易产生梯度爆炸。\n",
    "   循环神经网络中，更大的初始权重可能导致混沌现象：对于输入中的很小的扰动非常敏感，从而导致确定性算法给出了随机性结果。\n",
    "3. 关于如何初始化网络，正则化和最优化有两种不同的角度：\n",
    "   从最优化角度，建议权重应该足够大，从而能够成功传播信息。\n",
    "   从正则化角度，建议权重小一点（如 正则化），从而提高泛化能力。\n",
    "4. 实践中，通常需要将初始权重范围视作超参数。如果计算资源允许，可以将每层权重的初始数值范围设置为一个超参数，然后使用超参数搜索算法来挑选这些超参数。\n",
    "\n",
    "#### 5.1.2 偏置初始化\n",
    "\n",
    " 1. 偏置的初始化通常更容易。大多数情况下，可以设置偏置初始化为零。\n",
    "\n",
    " 2.  有时可以设置偏置初始化为非零，这发生在下面的三种情况：\n",
    "\n",
    "    如果偏置是作为输出单元，则初始化偏置为非零值。假设初始权重足够小，输出单元的输出仅由初始化偏置决定，则非零的偏置有助于获取正确的输出边缘统计。\n",
    "\n",
    "    有时选择偏置的初始值以免初始化引起激活函数饱和。如：ReLU 激活函数的神经元的偏置设置为一个小的正数，从而避免ReLU 初始时就位于饱和的区域。\n",
    "\n",
    "    有时某个单元作为开关来决定其他单元是使用还是不使用。此时偏置应该非零，从而打开开关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动量法（Momentum）\n",
    "该适用于隧道型曲面，梯度下降法在狭长的隧道型函数上表现不佳，如下图所示\n",
    "\n",
    "* 函数主体缓缓向右方下降\n",
    "* 在主体方向两侧各有一面高墙，导致垂直于主体方向有更大的梯度\n",
    "* 梯度下降法会在隧道两侧频繁震荡\n",
    "\n",
    "  而动量法每次更新都吸收一部分上次更新的余势。这样主体方向的更新就得到了更大的保留，从而效果被不断放大。物理上这就像是推一个很重的铁球下山，因为铁球保持了下山主体方向的动量，所以在隧道上沿两侧震荡测次数就会越来越少。\n",
    "\n",
    "## Adagrad\n",
    "该算法的特点是自动调整学习率，适用于稀疏数据。梯度下降法在每一步对每一个参数使用相同的学习率，这种一刀切的做法不能有效的利用每一个数据集自身的特点。\n",
    "\n",
    "  Adagrad 是一种自动调整学习率的方法：\n",
    "\n",
    "* 随着模型的训练，学习率自动衰减\n",
    "* 对于更新频繁的参数，采取较小的学习率\n",
    "* 对于更新不频繁的参数，采取较大的学习率\n",
    "## Adadelta(Adagrad的改进算法)\n",
    "Adagrad的一个问题在于随着训练的进行，学习率快速单调衰减。Adadelta则使用梯度平方的移动平均来取代全部历史平方和。\n",
    " \n",
    "Adadelta 的第一个版本也叫做 RMSprop，是Geoff Hinton独立于Adadelta提出来的。\n",
    "## Adam\n",
    "如果把Adadelta里面梯度的平方和看成是梯度的二阶矩，那么梯度本身的求和就是一阶矩。Adam算法在Adadelta的二次矩基础之上又引入了一阶矩。而一阶矩，其实就类似于动量法里面的动量。\n",
    "## Normalization\n",
    "### batch normalization\n",
    "1.batch normalization是优化神经网络的一大创新。\n",
    "* 它并不是一个优化算法，而是一个自适应的、调整参数模型的方法。\n",
    "* 它试图解决训练非常深的神经网络的困难。\n",
    "\n",
    "2.深度神经网络训练困难的一个重要原因是：深度神经网络涉及很多层的叠加，而每一层的参数更新会导致上一层的输入数据分布发生变化。这会带来两个问题：\n",
    "* 下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。\n",
    "\n",
    "* 通过层层叠加，高层的输入分布变化会非常剧烈。这就使得高层需要不断去适应底层的参数更新变化。这就要求我们需要非常谨慎的设定学习率、初始化权重、参数更新策略。\n",
    "\n",
    "### layer normalization\n",
    "与 BN 不同，LN 是对单个样本的同一层的神经元进行归一化，同层神经元使用相同的均值和方差。对于该层神经元，不同样本可以使用的均值和方差不同。\n",
    "\n",
    "与之相比，BN 是对每个神经元在mini batch 样本之间计算均值和方差。对每个神经元，mini batch 中的所有样本在该神经元上都使用相同的均值和方差。但是不同神经元使用不同的均值和方差。\n",
    "\n",
    "因此LN 不依赖于batch size，也不依赖于网络深度。因此它适合在线学习，也适合于RNN 网络。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
