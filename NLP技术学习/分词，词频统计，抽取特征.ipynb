{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基本文本处理技能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 分词的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管现在很多文本处理采用基于字/字符的方式，词作为能够独立语用的基本语言单位，依然是目前是主流的NLP任务的基本处理单位。对于没有间隔符的汉语，分词就成了文本预处理的第一个任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 基于规则分词包括： 正向最大匹配法，逆向最大匹配法，双向最大匹配法\n",
    "* 具体内容参考：https://blog.csdn.net/shark803/article/details/89189737"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 基于统计分词包括： HMM+Viterbi分词，CRF+Viterbi分词\n",
    "* HMM具体内容参考：https://www.cnblogs.com/Denise-hzf/p/6612212.html\n",
    "* CRF具体内容参考：http://www.cnblogs.com/en-heng/p/6214023.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 基于神经网络分词包括： bi-LSTM+Viterbi分词\n",
    "* 具体内容参考：https://www.cnblogs.com/Denise-hzf/p/6612212.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 开源的分词工具：jieba、thulac、snownlp、pkuseg、pynlpir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 词、字符频率统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 字符统计代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'爱': 2, '你': 2, '我': 1, '，': 1, '死': 1})\n"
     ]
    }
   ],
   "source": [
    "text = \"我爱你，爱死你\"\n",
    "from collections import Counter\n",
    "c = Counter(text)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 词频统计相比字符统计而言，只是多了一步分词的过程，具体代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'我爱你': 1, '，': 1, '爱死': 1, '你': 1})\n"
     ]
    }
   ],
   "source": [
    "import jieba \n",
    "text = \"我爱你，爱死你\"\n",
    "seg_list = list(jieba.cut(text, cut_all=False)) \n",
    "c = Counter(seg_list )\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 语言模型中unigram、bigram、trigram的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* unigram指的是单个词为一个单元。每个词之间没有关联关系。\n",
    "* bigram指的是两个词为一个单元。当前词只和上一个词有关系。\n",
    "* trigram指的是三个词为一个单元。当前词只和前两个词有关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 unigram、bigram频率统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以使用Python中的collections.Counter模块\n",
    "* unigram等同于词频统计，而bigram频率统计只需要取n-gram，然后传入到Counter()中即可。\n",
    "* sklearn CountVectorizer 可是设置n-gram参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 文本矩阵化：要求采用词袋模型且是词级别的矩阵化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 分词工具采用结巴中文分词，涉及到的算法：\n",
    "\n",
    "基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)；\n",
    "\n",
    "采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合；\n",
    "\n",
    "对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法。\n",
    "\n",
    "* 结巴中文分词支持的三种分词模式包括：\n",
    "\n",
    "精确模式：试图将句子最精确地切开，适合文本分析；\n",
    "\n",
    "全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义问题；\n",
    "\n",
    "搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 去停用词；构造词表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 中文停用词数量有限，网上一堆一堆的。\n",
    "* 英文停用词可以直接下载nltk的stopwords。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 构造词表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn.feature_extraction.text 的4中文本特征提取方法：\n",
    "\n",
    "* CounterVector\n",
    "* TfidfVectorizer\n",
    "* TfidfTransformer\n",
    "* HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 每篇文档的向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "count_vectorizer.fit(seg_list)\n",
    "vec = count_vectorizer.transform(seg_list).toarray()\n",
    "vocab_list = count_vectorizer.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
