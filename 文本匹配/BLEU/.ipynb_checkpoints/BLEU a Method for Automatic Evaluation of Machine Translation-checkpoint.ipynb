{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU: a Method for Automatic Evaluation of Machine Translation\n",
    "\n",
    "### BLEU：一种机器翻译自动评价方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 摘要\n",
    "\n",
    "* Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.\n",
    "\n",
    "* 人类对机器翻译的评价是广泛但昂贵的。人类评估可能需要几个月才能完成，并且涉及到无法重复使用的人力资源。我们提出了一种快速、廉价、独立于语言的机器翻译自动评估方法，它与人工评估高度相关，每次运行的边际成本很小。我们将此方法作为熟练的人类裁判的自动替补，当需要快速或频繁地进行评估时，可以代替他们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 介绍\n",
    "##### 理论基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Human evaluations of machine translation (MT) weigh many aspects of translation, including adequacy, fidelity , and fluency of the translation (Hovy, 1999; White and O’Connell, 1994). A comprehensive catalog of MT evaluation techniques and their rich literature is given by Reeder (2001). For the most part, these various human evaluation approaches are quite expensive (Hovy, 1999). Moreover, they can take weeks or months to finish. This is a big problem because developers of machine translation systems need to monitor the effect of daily changes to their systems in order to weed out bad ideas from good ideas. We believe that MT progress stems from evaluation and that there is a logjam of fruitful research ideas waiting to be released from the evaluation bottleneck. Developers would benefit from an inexpensive automatic evaluation that is quick, language-independent, and correlates highly with human evaluation. We propose such an evaluation method in this paper.\n",
    "\n",
    "* 人类对机器翻译的评价（MT）衡量翻译的许多方面，包括翻译的质量、忠实度和流畅性（Hovy，1999；White和O'Connell，1994）。Reeder（2001）提供了MT评估技术及其丰富文献的综合目录。在大多数情况下，这些不同的人类评估方法相当昂贵（Hovy，1999）。此外，他们可能需要数周或数月才能完成。这是一个大问题，因为机器翻译系统的开发人员需要监控系统的日常变化的影响，以便从好的想法中剔除坏的想法。我们相信机器翻译的进步源于评估，还有一大堆富有成效的研究想法等待着我们去释放"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 观点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How does one measure translation performance? The closer a machine translation is to a professional human translation, the better it is. This is the central idea behind our proposal. To judge the quality of a machine translation, one measures its closeness to one or more reference human translations according to a numerical metric. Thus, our MT evaluation system requires two ingredients:\n",
    "* 如何衡量翻译绩效？机器翻译离专业的人工翻译越近越好。这是我们提议的核心思想。为了判断机器翻译的质量，人们根据一个数字度量来衡量它与一个或多个参考人类翻译的接近程度。因此，我们的机器翻译评估系统需要两个要素："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. a numerical “translation closeness” metric\n",
    "* 2. a corpus of good quality human reference translations\n",
    "\n",
    "* 1一种数值“平移贴近度”度量\n",
    "\n",
    "* 2一个高质量的人类参考翻译语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We fashion our closeness metric after the highly successful word error rate metric used by the speech recognition community, appropriately modified for multiple reference translations and allowing for legitimate differences in word choice and word order. The main idea is to use a weighted average of variable length phrase matches against the reference translations. This view gives rise to a family of metrics using various weighting schemes. We have selected a promising baseline metric from this family.\n",
    "\n",
    "* 我们根据语音识别社区使用的非常成功的单词错误率度量来设计我们的贴近度度量，对多个引用翻译进行了适当的修改，并允许在单词选择和词序方面存在合理的差异。其主要思想是对参考译文使用可变长度短语匹配的加权平均值。这种观点产生了一系列使用不同权重方案的度量。我们从这个系列中选择了一个有前途的基线指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In Section 2, we describe the baseline metric in detail. In Section 3, we evaluate the performance of BLEU. In Section 4, we describe a human evaluation experiment. In Section 5, we compare our baseline metric performance with human evaluations.\n",
    "* 在第2节中，我们详细描述了基线度量。在第三部分，我们评估了BLEU的性能。在第四部分，我们描述了一个人体评价实验。在第5节中，我们将基线度量性能与人类评估进行比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 基线BLEU指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Typically, there are many “perfect” translations of a given source sentence. These translations may vary in word choice or in word order even when they use the same words. And yet humans can clearly distinguish a good translation from a bad one. For example, consider these two candidate translations of a Chinese source sentence:\n",
    "\n",
    "* 通常，一个给定的源句子有许多“完美”的翻译。即使使用相同的词，这些译文在选词或语序上也可能有所不同。然而，人类可以清楚地区分好的翻译和坏的翻译。例如，考虑一个中文源句的两个候选译文："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 例子1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Candidate 1: It is a guide to action which ensures that the military always obeys the commands of the party.\n",
    "\n",
    "2. Candidate 2: It is to insure the troops forever hearing the activity guidebook that party direct.\n",
    "\n",
    "* Although they appear to be on the same subject, they differ markedly in quality. For comparison, we provide three reference human      translations of the same sentence below.\n",
    "\n",
    "* 虽然它们看起来是在同一个主题上，但在质量上却有明显的不同。为了比较，我们提供了三个参考的相同句子的人工翻译。\n",
    "\n",
    "3. Reference 1: It is a guide to action that ensures that the military will forever heed Party commands.\n",
    "4. Reference 2: It is the guiding principle which guarantees the military forces always being under the command of the Party.\n",
    "5. Reference 3: It is the practical guide for the army always to heed the directions of the party.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is clear that the good translation, Candidate 1, shares many words and phrases with these three reference translations, while Candidate 2 does not. We will shortly quantify this notion of sharing in Section 2.1. But first observe that Candidate 1 shares \"It is a guide to action\" with Reference 1, \"which\" with Reference 2, \"ensures that the military\" with Reference 1, \"always\" with References 2 and 3, \"commands\" with Reference 1, and finally \"of the party\" with Reference 2 (all ignoring capitalization). In contrast, Candidate 2 exhibits far fewer matches, and their extent is less.\n",
    "\n",
    "* 很明显，好的译文，候选1，与这三个参考译文有很多相同的单词和短语，而候选2没有。我们将很快在第2.1节中量化这种共享的概念。但首先要注意的是，候选人1与参考文献1共享“这是行动指南”，即“参考文献2”，确保军队“参考文献1”，始终“参考文献2和3”，参考文献1“指挥”，最后参考文献2（均忽略大写字母）。相比之下，候选者2表现出的匹配要少得多，而且范围也更小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is clear that a program can rank Candidate 1 higher than Candidate 2 simply by comparing n-gram matches between each candidate translation and the reference translations. Experiments over large collections of translations presented in Section 5 show that this ranking ability is a general phenomenon, and not an artifact of a few toy examples.\n",
    "* 很明显，程序只需比较每个候选翻译和参考翻译之间的n元匹配，就可以将候选1比候选2高。在第5节中展示的大量翻译集上的实验表明，这种排名能力是一种普遍现象，而不是几个玩具例子的产物。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate translation is. For simplicity, we first focus on computing unigram matches.\n",
    "* BLEU实现器的主要编程任务是比较候选的n个gram和参考翻译的n个gram，并计算匹配的数量。这些匹配是位置无关的。匹配越多，候选译文就越好。为了简单起见，我们首先关注于计算unigram匹配。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1修正n-gram精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The cornerstone of our metric is the familiar precision measure. To compute precision, one simply counts up the number of candidate translation words (unigrams) which occur in any reference translation and then divides by the total number of words in the candidate translation. Unfortunately, MT systems can overgenerate “reasonable” words, resulting in improbable, but high-precision, translations like that of example 2 below. Intuitively the problem is clear: a reference word should be considered exhausted after a matching candidate word is identified. We formalize this intuition as the modified unigram precision. To compute this, one first counts the maximum number of times a word occurs in any single reference translation. Next, one clips the total count of each candidate word by its maximum reference count,2adds these clipped counts up, and divides by the total (unclipped) number of candidate words.\n",
    "* 我们度量的基础是我们熟悉的精度度量。为了计算精确性，我们只需将任何参考翻译中出现的候选翻译单词（unigram）的数量相加，然后除以候选翻译中的总单词数。不幸的是，机器翻译系统可能会过度生成“合理”的单词，导致不太可能的，但是高精度的翻译，如下面的例子2所示。直观地说，问题很明显：在识别出匹配的候选词之后，参考词应该被视为用尽。我们将这种直觉形式化为修正的单元精度。要计算这一点，首先计算一个词在任何单个引用翻译中出现的最大次数。接下来，将每个候选词的总计数除以其最大引用计数，2将这些剪裁的计数相加，然后除以候选词的总数（未压缩）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 例子2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Candidate: the the the the the the the. \n",
    "2. Reference 1: The cat is on the mat. \n",
    "3. Reference 2: There is a cat on the mat.\n",
    "\n",
    "    * 修正的单元精度 = 2/7^3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In Example 1, Candidate 1 achieves a modified unigram precision of 17/18; whereas Candidate 2 achieves a modified unigram precision of 8/14. Similarly, the modified unigram precision in Example 2 is 2/7, even though its standard unigram precision is 7/7.\n",
    "* 在实施例1中，候选项1实现了17/18的修正单元精度；而候选项2实现了8/14的修正单元精度。类似地，在实施例2中，修改后的unigram精度是2/7，即使其标准的unigram精度是7/7。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modified n-gram precision is computed similarly for any n: all candidate n-gram counts and their corresponding maximum reference counts are collected. The candidate counts are clipped by their corresponding reference maximum value, summed, and divided by the total number of candidate n-grams. In Example 1, Candidate 1 achieves a modified bigram precision of 10/17, whereas the lower quality Candidate 2 achieves a modified bigram precision of 1/13. In Example 2, the (implausible) candidate achieves a modified bigram precision of 0. This sort of modified n-gram precision scoring captures two aspects of translation: adequacy and fluency. A translation using the same words (1-grams) as in the references tends to satisfy adequacy. The longer n-gram matches account for fluency.\n",
    "* 对于任何n，修正的n-gram精度计算类似：收集所有候选n-gram计数及其相应的最大参考计数。候选计数由其相应的参考最大值剪裁，求和，然后除以候选n-gram的总数。在实例1中，候选项1实现了10/17的修正二元精度，而质量较低的候选项2实现了1/13的修正二元精度。在示例2中，（难以置信的）候选者实现了修改后的二元精度0。这种改进的n-gram精确评分法抓住了翻译的两个方面：充分性和流畅性。使用与参考文献中相同的单词（1-grams）的译文往往能满足充分性的要求。较长的n-gram匹配说明了流畅性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 文本块的改进n-gram精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do we compute modified n-gram precision on a multi-sentence test set? Although one typically evaluates MT systems on a corpus of entire documents, our basic unit of evaluation is the sentence. A source sentence may translate to many target sentences, in which case we abuse terminology and refer to the corresponding target sentences as a “sentence.” We first compute the n-gram matches sentence by sentence. Next, we add the clipped n-gram counts for all the candidate sentences and divide by the number of candidate n-grams in the test corpus to compute a modified precision score,pn, for the entire test corpus.\n",
    "* 如何在多句测试集上计算修正的n-gram精度？虽然人们通常是在一个完整的语料库上评估机器翻译系统，但我们的基本评估单位是句子。一个源句子可能会翻译成许多目标句子，在这种情况下，我们滥用术语，将对应的目标句子称为“句子”，我们首先逐句计算n-gram匹配。接下来，我们对所有候选句子加上裁剪后的n-gram计数，除以测试语料库中候选n-gram的数量，计算出整个测试语料库的修正精度分数pn。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](bleu/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 仅使用修正n-gram精度的排序系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To verify that modified n-gram precision distinguishes between very good translations and bad translations, we computed the modified precision numbers on the output of a (good) human translator and a standard (poor) machine translation system using 4 reference translations for each of 127 source sentences. The average precision results are shown in Figure 1.\n",
    "* 为了验证修正的n-gram精度能区分非常好的翻译和糟糕的翻译，我们计算了一个（好的）人工翻译和一个标准（差）机器翻译系统输出的修正精度数，每个127个源句子使用4个参考译文。平均精度结果如图1所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>图1：区分人与机器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](bleu/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The strong signal differentiating human (high precision) from machine (low precision) is striking. The difference becomes stronger as we go from unigram precision to 4-gram precision. It appears that any single n-gram precision score can distinguish between a good translation and a bad translation. To be useful, however, the metric must also reliably distinguish between translations that do not differ so greatly in quality. Furthermore, it must distinguish between two human translations of differing quality. This latter requirement ensures the continued validity of the metric as MT approaches human translation quality.\n",
    "* 区分人（高精度）和机器（低精度）的强烈信号引人注目。当我们从unigram到4-gram时，差异变得更大。似乎任何一个n-gram的精度分数都可以区分好的翻译和坏的翻译。然而，为了有用，该度量还必须可靠地区分质量差别不大的翻译。此外，它还必须区分两个质量不同的人工翻译。后一个要求确保了当机器翻译接近人工翻译质量时，度量标准的持续有效性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To this end, we obtained a human translation by someone lacking native proficiency in both the source (Chinese) and the target language (English). For comparison, we acquired human translations of the same documents by a native English speaker. We also obtained machine translations by three commercial systems. These five “systems” — two humans and three machines — are scored against two reference professional human translations. The average modified n-gram precision results are shown in Figure 2.\n",
    "* 为此，我们获得了一个由缺乏母语（中文）和目标语言（英语）的人翻译的。为了进行比较，我们获得了一个以英语为母语的人翻译的相同文件。我们还获得了三个商业系统的机器翻译。这五个“系统”——两个人类和三个机器——是对两个参考专业的人工翻译进行评分的。平均修正n-gram精度结果如图2所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each of these n-gram statistics implies the same ranking: H2 (Human-2) is better than H1 (Human-1), and there is a big drop in quality between H1 and S3 (Machine/System-3). S3 appears better than S2 which in turn appears better than S1. Remarkably, this is the same rank order assigned to these “systems” by human judges, as we discuss later. While there seems to be ample signal in any single n-gram precision, it is more robust to combine all these signals into a single number metric.\n",
    "* 每一个n-gram统计都意味着相同的排名：H2（人-2）比H1（人-1）好，而且H1和S3（机器/系统-3）之间的质量有很大的下降。S3比S2好，而S2又比S1好。值得注意的是，正如我们后面讨论的那样，这是人类法官赋予这些“系统”的相同的等级顺序。虽然在任何一个n-gram精度中似乎都有足够的信号，但是将所有这些信号组合到一个单一的数字度量中会更加健壮。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>图2：机器和人工翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](bleu/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3结合改进的n-gram精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How should we combine the modified precisions for the various n-gram sizes? A weighted linear average of the modified precisions resulted in encouraging results for the 5 systems. However, as can be seen in Figure 2, the modified n-gram precision decays roughly exponentially with n: the modified unigram precision is much larger than the modified bigram precision which in turn is much bigger than the modified trigram precision. A reasonable averaging scheme must take this exponential decay into account; a weighted average of the logarithm of modified precisions satisifies this requirement.\n",
    "* 我们应该如何组合各种n-gram大小的修正精度？修正精度的加权线性平均值对5个系统产生了令人鼓舞的结果。然而，如图2所示，修改后的n元精度随n大致呈指数衰减：修改后的单字图精度远远大于修改后的二元图精度，后者又远远大于修改后的三元曲线精度。一个合理的平均方案必须考虑到这种指数衰减；修正精度对数的加权平均满足这一要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BLEU uses the average logarithm with uniform weights, which is equivalent to using the geometric mean of the modified n-gram precisions.5,6 Experimentally, we obtain the best correlation with monolingual human judgments using a maximum n-gram order of 4, although 3-grams and 5-grams give comparable results.\n",
    "* BLEU使用具有均匀权重的平均对数，这相当于使用修正n-gram精度的几何平均值。5,6实验中，我们使用最大n-gram阶数4获得与单语人类判断的最佳相关性，尽管3-gram和5-gram给出了可比的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2句子长度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A candidate translation should be neither too long nor too short, and an evaluation metric should enforce this. To some extent, the n-gram precision already accomplishes this. N-gram precision penalizes spurious words in the candidate that do not appear in any of the reference translations. Additionally, modified precision is penalized if a word occurs more frequently in a candidate translation than its maximum reference count. This rewards using a word as many times as warranted and penalizes using a word more times than it occurs in any of the references. However, modified n-gram precision alone fails to enforce the proper translation length, as is illustrated in the short, absurd example below.\n",
    "* 候选译文既不应太长也不应太短，评估指标应强制执行这一点。在某种程度上，n-gram精度已经实现了这一点。N-gram precision惩罚候选词中没有出现在任何参考译文中的伪词。此外，如果一个词在候选译文中出现的频率高于其最大引用计数，则修改后的精度将受到惩罚。这将奖励使用一个单词的次数，并惩罚使用一个单词的次数超过它在任何引用中出现的次数。然而，修正的n-gram精度本身并不能保证正确的翻译长度，下面的一个荒谬的简短例子就说明了这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 例子3："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Candidate: of the\n",
    "2. Reference 1: It is a guide to action that ensures that the military will forever heed Party commands.\n",
    "3. Reference 2: It is the guiding principle which guarantees the military forces always being under the command of the Party.\n",
    "4. Reference 3: It is the practical guide for the army always to heed the directions of the party."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Because this candidate is so short compared to the proper length, one expects to find inflated precisions: the modified unigram precision is 2/2, and the modified bigram precision is 1/1.\n",
    "* 因为这个候选者与合适的长度相比太短了，所以我们期望找到膨胀的精度：修正的单峰精度是2/2，修正的二元精度是1/1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 召回的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Traditionally, precision has been paired with recall to overcome such length-related problems. However, BLEU considers multiple reference translations, each of which may use a different word choice to translate the same source word. Furthermore, a good candidate translation will only use (recall) one of these possible choices, but not all. Indeed, recalling all choices leads to a bad translation. Here is an example.\n",
    "* 传统上，精确与召回相结合，以克服这种长度相关的问题。然而，BLEU考虑多个引用翻译，每个引用翻译可能使用不同的单词选择来翻译同一个源单词。此外，一个好的候选翻译只会使用（回忆）这些可能的选择之一，而不是全部。事实上，回忆所有的选择都会导致糟糕的翻译。这里有一个例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 例子4："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Candidate 1:ally do.\n",
    "2. Candidate 2: Reference 1: Reference 2: Reference 3:\n",
    "3. I always invariably perpetu-\n",
    "4. I always do.\n",
    "5. I always do.\n",
    "6. I invariably do. I perpetually do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first candidate recalls more words from the references, but is obviously a poorer translation than the second candidate. Thus, na ̈ıve recall computed over the set of all reference words is not a good measure. Admittedly, one could align the reference translations to discover synonymous words and compute recall on concepts rather than words. But, given that reference translations vary in length and differ in word order and syntax, such a computation is complicated.\n",
    "* 第一个候选者从参考文献中回忆起更多的单词，但显然比第二个候选者差。因此，在所有参考词集合上计算自然回忆并不是一个好的衡量标准。诚然，人们可以调整参考译文来发现同义词，并计算对概念的回忆而不是单词。但是，鉴于参考译文的长度不同，语序和句法也不同，这样的计算就很复杂了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 句子简短处罚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Candidate translations longer than their references are already penalized by the modified n-gram precision measure: there is no need to penalize them again. Consequently, we introduce a multiplicative brevity penalty factor. With this brevity penalty in place, a high-scoring candidate translation must now match the reference translations in length, in word choice, and in word order. Note that neither this brevity penalty nor the modified n-gram precision length effect directly considers the source length; instead, they consider the range of reference translation lengths in the target language.\n",
    "* 比参考文献更长的候选译文已经被修正的n-gram精度度量所惩罚：没有必要再次惩罚它们。因此，我们引入了一个乘法简洁惩罚因子。有了这种简洁的惩罚，高分候选译文现在必须在长度、选词和语序上与参考译文相匹配。请注意，这种简洁性惩罚和修改后的n-gram精度长度效应都没有直接考虑源代码长度；相反，它们考虑目标语言中引用翻译长度的范围。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We wish to make the brevity penalty 1.0 when the candidate’s length is the same as any reference translation’s length. For example, if there are three references with lengths 12, 15, and 17 words and the candidate translation is a terse 12 words, we want the brevity penalty to be 1. We call the closest reference sentence length the “best match length.”\n",
    "* 当候选人的长度与任何参考译文的长度相同时，我们希望使简洁性惩罚为1.0。例如，如果有三个参考文献的长度分别为12、15和17个单词，并且候选译文是一个简洁的12个单词，我们希望简洁性惩罚为1。最接近的句子长度称为“最接近的长度”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One consideration remains: if we computed the brevity penalty sentence by sentence and averaged the penalties, then length deviations on short sentences would be punished harshly. Instead, we compute the brevity penalty over the entire corpus to allow some freedom at the sentence level. We first compute the test corpus’ effective reference length, r, by summing the best match lengths for each candidate sentence in the corpus. We choose the brevity penalty to be a decaying exponential in r/c, where c is the total length of the candidate translation corpus.\n",
    "* 一个需要考虑的问题是：如果我们逐句计算简短句的惩罚，并将其平均化，那么短句的长度偏差将受到严厉的惩罚。相反，我们计算整个语料库的简洁性惩罚，以便在句子层面上允许一些自由。我们首先计算出测试语料库的有效参考长度r，计算出语料库中每个候选句子的最佳匹配长度。我们选择简洁性惩罚为r/c中的衰减指数，其中c是候选翻译语料库的总长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 BLEU 细节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We take the geometric mean of the test corpus’modified precision scores and then multiply the result by an exponential brevity penalty factor. Currently, case folding is the only text normalization performed before computing the precision.\n",
    "* 我们取测试语料库修正精度得分的几何平均值，然后将结果乘以指数简洁性惩罚因子。目前，在计算精度之前，唯一执行的文本标准化是大小写折叠。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We first compute the geometric average of the modified n-gram precisions, pn, using n-grams up to length N and positive weights wn summing to one.\n",
    "* 我们首先计算修正的n-gram精度pn的几何平均值，使用n-gram的长度n和正权值wn之和为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, let c be the length of the candidate translation and r be the effective reference corpus length. We compute the brevity penalty BP,\n",
    "* 其次，c为候选译文的长度，r为有效参考语料库长度。我们计算简洁性惩罚BP，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](bleu/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 然后"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](bleu/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The ranking behavior is more immediately apparent in the log domain,\n",
    "* 在日志域中排名行为更为明显，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](bleu/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In our baseline, we use N = 4 and uniform weights wn = 1/N.\n",
    "* 在我们的基线中，我们使用N=4和均匀权重wn=1/N。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 BLEU评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The BLEU metric ranges from 0 to 1. Few translations will attain a score of 1 unless they are identical to a reference translation. For this reason, even a human translator will not necessarily score 1. It is important to note that the more reference translations per sentence there are, the higher the score is. Thus, one must be cautious making even “rough” comparisons on evaluations with different numbers of reference translations: on a test corpus of about 500 sentences (40 general news stories), a human translator scored 0.3468 against four references and scored 0.2571 against two references. Table 1 shows the BLEU scores of the 5 systems against two references on this test corpus.\n",
    "* BLEU度量的范围是0到1。除非译文与参考译文相同，否则很少有译文能得1分。因此，即使是人工翻译也不一定能得1分。值得注意的是，每个句子的参考译文越多，得分就越高。因此，我们必须谨慎地对不同参考译文数量的评价进行“粗略”的比较：在一个大约500句话（40个一般新闻报道）的测试语料库中，一个人类译者对四个参考文献的评分为0.3468，而对两个参考文献的评分为0.2571。表1显示了这5个系统的BLEU分数与两个参考文献在这个测试语料库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The MT systems S2 and S3 are very close in this metric. Hence, several questions arise:\n",
    "* MT系统S2和S3在这个指标中非常接近。因此，出现了几个问题："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](bleu/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is the difference in BLEU metric reliable?\n",
    "* What is the variance of the BLEU score?\n",
    "* If we were to pick another random set of 500 sentences, would we still judge S3 to be better than S2?\n",
    "--------\n",
    "* BLEU度量的差异是否可靠？\n",
    "* BLEU分数的方差是多少？\n",
    "* 如果我们再随机抽取500个句子，我们还会判断S3比S2好吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To answer these questions, we divided the test corpus into 20 blocks of 25 sentences each, and computed the BLEU metric on these blocks individually. We thus have 20 samples of the BLEU metric for each system. We computed the means, variances, and paired t-statistics which are displayed in Table 2. The t-statistic compares each system with its left neighbor in the table. For example, t = 6 for the pair S1 and S2.\n",
    "* 为了回答这些问题，我们将测试语料分成20个块，每个块25个句子，并分别计算这些块上的BLEU度量。因此，对于每个系统，我们有20个BLEU度量的样本。我们计算了平均数、方差和配对t统计量，如表2所示。t统计量将每个系统与其表中的左邻居进行比较。例如，对于S1和S2，t=6。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that the numbers in Table 1 are the BLEU metric on an aggregate of 500 sentences, but the means in Table 2 are averages of the BLEU metric on aggregates of 25 sentences. As expected, these two sets of results are close for each system and differ only by small finite block size effects. Since a paired t-statistic of 1.7 or above is 95% significant, the differences between the systems’ scores are statistically very significant. The reported variance on 25-sentence blocks serves as an upper bound to the variance of sizeable test sets like the 500 sentence corpus.\n",
    "* 请注意，表1中的数字是总计500个句子的BLEU度量，但表2中的平均值是25个句子集合的BLEU度量的平均值。正如预期的那样，这两组结果对于每一个系统来说都很接近，只是在有限块尺寸效应上有所不同。由于1.7或以上的配对t统计量是95%的显著性，所以系统得分之间的差异在统计学上是非常显著的。所报告的25个句子块的方差是相当大的测试集（如500个句子语料库）方差的上界。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many reference translations do we need? We simulated a single-reference test corpus by randomly selecting one of the 4 reference translations as the single reference for each of the 40 stories. In this way, we ensured a degree of stylistic variation. The systems maintain the same rank order as with multiple references. This outcome suggests that we may use a big test corpus with a single reference translation, provided that the translations are not all from the same translator.\n",
    "* 我们需要多少参考译文？我们通过随机选择4个参考译文中的一个作为40个故事的单个参考，模拟了一个单一的参考测试语料库。通过这种方式，我们确保了一定程度的风格变化。系统保持与多个引用相同的秩序。这一结果表明，我们可以使用一个大测试语料库和一个单一的参考翻译，前提是翻译不是所有来自同一个译者。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 人类评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We had two groups of human judges. The first group, called the monolingual group, consisted of 10 native speakers of English. The second group, called the bilingual group, consisted of 10 native speakers of Chinese who had lived in the United States for the past several years. None of the human judges was a professional translator. The humans judged our 5 standard systems on a Chinese sentence subset extracted at random from our 500 sentence test corpus. We paired each source sentence with each of its 5 translations, for a total of 250 pairs of Chinese source and English translations. We prepared a web page with these translation pairs randomly ordered to disperse the five translations of each source sentence. All judges used this same webpage and saw the sentence pairs in the same order. They rated each translation from 1 (very bad) to 5 (very good). The monolingual group made their judgments based only on the translations’ readability and fluency.\n",
    "* 我们有两组人类裁判。第一组被称为单语组，由10名以英语为母语的人组成。第二组被称为双语组，由10名在美国生活了几年的以汉语为母语的人组成。人类评委中没有一个是专业翻译。人类根据从我们的500个句子测试语料库中随机抽取的一个中文句子子集来判断我们的5个标准系统。我们将每一个源句与它的5个译文进行配对，总共有250对中文源语和英语译文。我们准备了一个网页，这些翻译对随机排列，分散了每个源句的五个翻译。所有的评委都使用同一个网页，看到的句子是按相同的顺序排列的。他们把每一个翻译从1分（非常差）到5分（非常好）。单语组只根据译文的可读性和流畅性来判断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As must be expected, some judges were more liberal than others. And some sentences were easier to translate than others. To account for the intrinsic difference between judges and the sentences, we compared each judge’s rating for a sentence across systems. We performed four pairwise t-test comparisons between adjacent systems as ordered by their aggregate average score.\n",
    "* 正如所料，有些法官比其他法官更为自由。有些句子比其他句子更容易翻译。为了解释法官和判决之间的内在差异，我们比较了不同系统中每个法官对一个判决的评分。我们在相邻系统之间进行了四个配对t检验比较，按照它们的总平均分数排序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 单语组成对判断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Figure 3 shows the mean difference between the scores of two consecutive systems and the 95% confidence interval about the mean. We see that S2 is quite a bit better than S1 (by a mean opinion score difference of 0.326 on the 5-point scale), while S3 is judged a little better (by 0.114). Both differences are significant at the 95% level.7 The human H1 is much better than the best system, though a bit worse than human H2. This is not surprising given that H1 is not a native speaker of either Chinese or English, whereas H2 is a native English speaker. Again, the difference between the human translators is significant beyond the 95% level.\n",
    "* 图3显示了两个连续系统得分之间的平均差和关于平均值的95%置信区间。我们发现S2比S1好很多（5分制的平均意见得分差为0.326），而S3则稍好一些（0.114）。这两种差异在95%的水平上都是显著的。7人类H1比最好的系统要好得多，尽管比人类的H2差一点。这并不奇怪，因为H1不是以汉语或英语为母语的人，而H2是以英语为母语的人。同样，在95%以上的水平上，人类译者之间的差异是显著的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>图3：单语判断-成对差异比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](bleu/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 双语群体成对判断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Figure 4 shows the same results for the bilingual group. They also find that S3 is slightly better than S2 (at 95% confidence) though they judge that the human translations are much closer (indistinguishable at 95% confidence), suggesting that the bilinguals tended to focus more on adequacy than on fluency.\n",
    "* 图4显示了双语组的相同结果。他们还发现，S3略好于S2（95%置信度），尽管他们判断人工翻译更接近（在95%置信度下无法区分），这表明双语者倾向于更注重充分性而不是流利性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>图4：双语判断-成对差异比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](bleu/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 BLEU与人类评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Figure 5 shows a linear regression of the monolingual group scores as a function of the BLEU score over two reference translations for the 5 systems. The high correlation coefficient of 0.99 indicates that BLEU tracks human judgment well. Particularly interesting is how well BLEU distinguishes between S2 and S3 which are quite close. Figure 6 shows the comparable regression results for the bilingual group. The correlation coefficient is 0.96.\n",
    "* 图5显示了单语组分数与5个系统的两个参考翻译的BLEU分数的函数的线性回归。高相关系数为0.99表明BLEU很好地跟踪了人类的判断。特别有趣的是BLEU如何很好地区分S2和S3，它们非常接近。图6显示了双语组的可比回归结果。相关系数为0.96。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>图5:BLEU预测单语判断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](bleu/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>图6:BLEU预测双语判断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](bleu/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We now take the worst system as a reference point and compare the BLEU scores with the human judgment scores of the remaining systems relative to the worst system. We took the BLEU, monolingual group, and bilingual group scores for the 5 systems and linearly normalized them by their corresponding range (the maximum and minimum score across the 5 systems). The normalized scores are shown in Figure 7. This figure illustrates the high correlation between the BLEU score and the monolingual group. Of particular interest is the accuracy of BLEU’s estimate of the small difference between S2 and S3 and the larger difference between S3 and H1. The figure also highlights the relatively large gap between MT systemsandhumantranslators.8 Inaddition,wesurmise that the bilingual group was very forgiving in judging H1 relative to H2 because the monolingual group found a rather large difference in the fluency of their translations.\n",
    "* 我们现在以最差系统为参考点，将BLEU分数与其余系统相对于最差系统的人类判断得分进行比较。我们取5个系统的BLEU、单语组和双语组的分数，并按其相应的范围（5个系统的最大和最小分数）对其进行线性归一化。标准化得分如图7所示。这张图说明了BLEU评分和单语组之间的高度相关性。特别令人感兴趣的是BLEU估计S2和S3之间的小差异和S3和H1之间的较大差异的准确性。该图还突出了机器翻译系统和人工翻译之间相对较大的差距。8此外，我们还发现，双语组在判断H1与H2时非常宽容，因为单语组在翻译流利性方面发现了相当大的差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>图7：BLEU与双语和单语判断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](bleu/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We believe that BLEU will accelerate the MT R&D cycle by allowing researchers to rapidly home in on effective modeling ideas. Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)! BLEU’s strength is that it correlates highly with human judgments by averaging out individual sentence judgment errors over a test corpus rather than attempting to divine the exact human judgment for every sentence: quantity leads to quality.\n",
    "* 我们相信，BLEU将通过让研究人员迅速掌握有效的建模思想，加快MT研发周期。我们的信念得到了最近一项统计分析的支持：BLEU与人类对翻译成英语的判断之间的相关性进行了统计分析，这些语言来自四种完全不同的语言（阿拉伯语、汉语、法语、西班牙语），它们代表了3种不同的语系（Papineni等人，2002年）！BLEU的优势在于它与人类的判断高度相关，它通过在测试语料库中平均出单个句子的判断错误，而不是试图预测每个句子的确切人类判断：数量导致质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, since MT and summarization can both be viewed as natural language generation from a textual context, we believe BLEU could be adapted to evaluating summarization or similar NLG tasks.\n",
    "* 最后，我们可以认为两种任务都可以被看作是一个自然的文本摘要，或者说是一个文本摘要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Acknowledgments This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No. N66001-99-2-8916. The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.\n",
    "* 承认这项工作得到了国防部高级研究项目局的部分支持，并由SPAWAR根据合同号N66001-99-2-8916进行了监控。本材料所载的观点和调查结果是作者的观点和发现，并不一定反映政府的政策立场，不应推断官方认可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We gratefully acknowledge comments about the geometric mean by John Makhoul of BBN and discussions with George Doddington of NIST. We especially wish to thank our colleagues who served in the monolingual and bilingual judge pools for their perseverance in judging the output of Chinese-English MT systems.\n",
    "* 我们非常感谢BBN的John Makhoul对几何平均值的评论以及与NIST的George Doddington的讨论。我们特别要感谢在单语和双语评委库工作的同事们，感谢他们坚持不懈地评判中英文机器翻译系统的输出。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
