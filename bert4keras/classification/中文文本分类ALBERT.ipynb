{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from bert4keras.backend import keras, set_gelu\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import open\n",
    "from keras.layers import Lambda, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 10 ['1@1', '1@307', '1@516', '2@301', '2@302', '2@304', '2@305', '2@306', '2@308', '2@309']\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename1,filename2):\n",
    "    \"\"\"加载数据\n",
    "    单条格式：(文本, 标签id)\n",
    "    \"\"\"\n",
    "    data1 = pd.read_excel(filename1)\n",
    "    data2 = pd.read_excel(filename2)\n",
    "    \n",
    "    label_set = list(set(list(data1['label'])+list(data2['label'])))\n",
    "    label_set.sort()\n",
    "    print(\"num_classes:\",len(label_set),label_set)\n",
    "    id_label = {}\n",
    "    for i in range(len(label_set)):\n",
    "        id_label[label_set[i]] = i\n",
    "    data1['label_id'] = data1['label'].map(id_label)\n",
    "    data2['label_id'] = data2['label'].map(id_label)\n",
    "    \n",
    "    D1 = []\n",
    "    for index,row in data1.iterrows():\n",
    "        D1.append((row['session_content'], int(row['label_id'])))\n",
    "    D2 = []\n",
    "    for index,row in data2.iterrows():\n",
    "        D2.append((row['session_content'], int(row['label_id'])))\n",
    "    return D1,D2,label_set\n",
    "\n",
    "# 加载数据集\n",
    "train_data,valid_data,label_list = load_data('traindata.xlsx','validdata.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 128)    2704384     Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 128)    256         Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 128)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 128)    65536       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 128)    256         Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Mapping (Dense)       (None, None, 384)    49536       Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-MultiHeadSelfAttent (None, None, 384)    591360      Embedding-Mapping[0][0]          \n",
      "                                                                 Embedding-Mapping[0][0]          \n",
      "                                                                 Embedding-Mapping[0][0]          \n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-MultiHeadSelfAttent (None, None, 384)    0           Embedding-Mapping[0][0]          \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[0][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[1][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[2][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[3][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward-Norm[4][0\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-MultiHeadSelfAttent (None, None, 384)    768         Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-FeedForward (FeedFo (None, None, 384)    1181568     Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-FeedForward-Add (Ad (None, None, 384)    0           Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[0][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[1][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[2][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[3][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[4][0]    \n",
      "                                                                 Transformer-MultiHeadSelfAttentio\n",
      "                                                                 Transformer-FeedForward[5][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-FeedForward-Norm (L (None, None, 384)    768         Transformer-FeedForward-Add[0][0]\n",
      "                                                                 Transformer-FeedForward-Add[1][0]\n",
      "                                                                 Transformer-FeedForward-Add[2][0]\n",
      "                                                                 Transformer-FeedForward-Add[3][0]\n",
      "                                                                 Transformer-FeedForward-Add[4][0]\n",
      "                                                                 Transformer-FeedForward-Add[5][0]\n",
      "__________________________________________________________________________________________________\n",
      "CLS-token (Lambda)              (None, 384)          0           Transformer-FeedForward-Norm[5][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 10)           3850        CLS-token[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,598,282\n",
      "Trainable params: 4,598,282\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_gelu('tanh')  # 切换gelu版本\n",
    "num_classes = len(label_list)\n",
    "maxlen = 512\n",
    "batch_size = 32\n",
    "\n",
    "# 下载预训练权重：https://github.com/brightmart/albert_zh\n",
    "config_path = 'albert_small_zh/albert_config_small_google.json'\n",
    "checkpoint_path = 'albert_small_zh/albert_model.ckpt'\n",
    "dict_path = 'albert_small_zh/vocab.txt'\n",
    "\n",
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        for is_end, (text, label) in self.sample(random):\n",
    "            token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_labels.append([label])\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_labels = sequence_padding(batch_labels)\n",
    "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
    "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "\n",
    "# 转换数据集\n",
    "train_generator = data_generator(train_data, batch_size)\n",
    "valid_generator = data_generator(valid_data, batch_size)\n",
    "\n",
    "# 建立分词器\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
    "\n",
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        for is_end, (text, label) in self.sample(random):\n",
    "            token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_labels.append([label])\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_labels = sequence_padding(batch_labels)\n",
    "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
    "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "\n",
    "\n",
    "# 加载预训练模型\n",
    "bert = build_transformer_model(\n",
    "    config_path=config_path,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    model='albert',\n",
    "    return_keras_model=False,\n",
    ")\n",
    "\n",
    "output = Lambda(lambda x: x[:, 0], name='CLS-token')(bert.model.output)\n",
    "output = Dense(\n",
    "    units=num_classes,\n",
    "    activation='softmax',\n",
    "    kernel_initializer=bert.initializer\n",
    ")(output)\n",
    "\n",
    "model = keras.models.Model(bert.model.input, output)\n",
    "model.summary()\n",
    "\n",
    "# 派生为带分段线性学习率的优化器。\n",
    "# 其中name参数可选，但最好填入，以区分不同的派生优化器。\n",
    "AdamLR = extend_with_piecewise_linear_lr(Adam, name='AdamLR')\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(1e-5),  # 用足够小的学习率\n",
    "#     optimizer=AdamLR(learning_rate=1e-4, lr_schedule={\n",
    "#         1000: 1,\n",
    "#         2000: 0.1\n",
    "#     }),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "def evaluate(data):\n",
    "    total, right = 0., 0.\n",
    "    for x_true, y_true in data:\n",
    "        y_pred = model.predict(x_true).argmax(axis=1)\n",
    "        y_true = y_true[:, 0]\n",
    "        total += len(y_true)\n",
    "        right += (y_true == y_pred).sum()\n",
    "    return right / total\n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    \"\"\"评估与保存\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.best_val_acc = 0.\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_acc = evaluate(valid_generator)\n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            model.save_weights('best_model.weights')\n",
    "#         test_acc = evaluate(test_generator)\n",
    "        print(\n",
    "            u'val_acc: %.5f, best_val_acc: %.5f\\n' %\n",
    "            (val_acc, self.best_val_acc)\n",
    "        )\n",
    "\n",
    "if False:\n",
    "    evaluator = Evaluator()\n",
    "    model.fit(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=20,\n",
    "        callbacks=[evaluator]\n",
    "    )\n",
    "    model.load_weights('best_model.weights')\n",
    "\n",
    "else:\n",
    "\n",
    "    model.load_weights('best_model.weights')\n",
    "#     val_acc = evaluate(valid_generator)\n",
    "#     print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm pb/ner_model.pb\n",
    "!rm -rf saved_model1/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is : ('Input-Token:0', 'Input-Segment:0')\n",
      "output is: dense_7/Softmax:0\n",
      "WARNING:tensorflow:From <ipython-input-5-b6eca674c6c7>:15: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 30 variables.\n",
      "INFO:tensorflow:Converted 30 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "# convert .h5 to .pb\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def,output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "print('input is :', (model.input[0].name,model.input[1].name))\n",
    "print ('output is:', model.output.name)\n",
    "sess = K.get_session()\n",
    "frozen_graph = freeze_session(K.get_session(), output_names=[model.output.op.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./pb/ner_model.pb'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.framework import graph_io\n",
    "\n",
    "output_path='./pb'\n",
    "pb_model_name='ner_model.pb'\n",
    "# pb_model_name='t5/t5_model.pb'\n",
    "graph_io.write_graph(frozen_graph, output_path, pb_model_name, as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "# export_dir = 't5/saved_model'\n",
    "# # graph_pb = './5_trained_model.pb'\n",
    "# graph_pb = 't5/t5_model.pb'\n",
    "export_dir = './saved_model1'\n",
    "# graph_pb = './5_trained_model.pb'\n",
    "graph_pb = './pb/ner_model.pb'\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "with tf.gfile.GFile(graph_pb, \"rb\") as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp0 Tensor(\"Input-Token:0\", shape=(?, ?), dtype=float32)\n",
      "inp1 Tensor(\"Input-Segment:0\", shape=(?, ?), dtype=float32)\n",
      "out Tensor(\"dense_7/Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./saved_model1/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'./saved_model1/saved_model.pb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigs = {}\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    tf.import_graph_def(graph_def, name=\"\")\n",
    "    g = tf.get_default_graph()\n",
    "    inp0 = g.get_tensor_by_name(model.input[0].name)\n",
    "    inp1 = g.get_tensor_by_name(model.input[1].name)\n",
    "    out = g.get_tensor_by_name(model.output.name)\n",
    "    print('inp0',inp0)\n",
    "    print('inp1',inp1)\n",
    "    print('out',out)\n",
    "    \n",
    "    sigs[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = tf.saved_model.signature_def_utils.predict_signature_def({\"in0\":inp0,\"in1\":inp1},{\"out\":out})\n",
    "    builder.add_meta_graph_and_variables(sess,[tag_constants.SERVING],signature_def_map=sigs)\n",
    "builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
